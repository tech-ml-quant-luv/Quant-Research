{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fd9ad39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from model_module import model_preparation, best_model\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "248a4b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-05 09:45:00+05:30</th>\n",
       "      <td>275.0</td>\n",
       "      <td>275.00</td>\n",
       "      <td>275.00</td>\n",
       "      <td>275.0</td>\n",
       "      <td>306762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-05 10:00:00+05:30</th>\n",
       "      <td>275.0</td>\n",
       "      <td>275.00</td>\n",
       "      <td>275.00</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-05 10:15:00+05:30</th>\n",
       "      <td>272.6</td>\n",
       "      <td>292.35</td>\n",
       "      <td>257.00</td>\n",
       "      <td>278.6</td>\n",
       "      <td>2886820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-05 10:30:00+05:30</th>\n",
       "      <td>278.6</td>\n",
       "      <td>292.40</td>\n",
       "      <td>275.00</td>\n",
       "      <td>286.7</td>\n",
       "      <td>949825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-05 10:45:00+05:30</th>\n",
       "      <td>286.7</td>\n",
       "      <td>294.80</td>\n",
       "      <td>285.55</td>\n",
       "      <td>294.0</td>\n",
       "      <td>831097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            open    high     low  close   volume\n",
       "datetime                                                        \n",
       "2020-10-05 09:45:00+05:30  275.0  275.00  275.00  275.0   306762\n",
       "2020-10-05 10:00:00+05:30  275.0  275.00  275.00  275.0        0\n",
       "2020-10-05 10:15:00+05:30  272.6  292.35  257.00  278.6  2886820\n",
       "2020-10-05 10:30:00+05:30  278.6  292.40  275.00  286.7   949825\n",
       "2020-10-05 10:45:00+05:30  286.7  294.80  285.55  294.0   831097"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/angelone_eq_15min_2018_2025.csv\").set_index(\"datetime\").drop(columns=[\"timestamp\"])\n",
    "# Convert df index to datetime (add this RIGHT AFTER loading/creating df)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a95365",
   "metadata": {},
   "source": [
    "#### Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4fea9",
   "metadata": {},
   "source": [
    "    Directional momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5fcde722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend direction\n",
    "df['green_candle'] = (df['close'] > df['open']).astype(int)\n",
    "df['higher_high'] = (df['high'] > df['high'].shift(1)).astype(int)\n",
    "df['lower_low'] = (df['low'] < df['low'].shift(1)).astype(int)\n",
    "df['close_above_sma13'] = (df['close'] > df['close'].rolling(13).mean()).astype(int)\n",
    "df['close_above_sma26'] = (df['close'] > df['close'].rolling(26).mean()).astype(int)\n",
    "\n",
    "# # Momentum direction\n",
    "# df['return_1_sign'] = np.sign(df['log_return'].shift(1))\n",
    "# df['return_4_positive'] = (df['return_4'] > 0).astype(int)\n",
    "# df['return_13_positive'] = (df['return_13'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "76c30e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features (datetime is index)\n",
    "df['hour'] = pd.to_datetime(df.index).hour\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week'] = pd.to_datetime(df.index).dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b51a5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hl_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "df['upper_shadow'] = (df['high'] - df[['open', 'close']].max(axis=1)) / df['close']\n",
    "df['lower_shadow'] = (df[['open', 'close']].min(axis=1) - df['low']) / df['close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40851d",
   "metadata": {},
   "source": [
    "    momentum Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9e0b488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['return_4'] = np.log(df['close'] / df['close'].shift(4))   # 1 hour\n",
    "df['return_13'] = np.log(df['close'] / df['close'].shift(13)) # ~3 hours\n",
    "df['return_26'] = np.log(df['close'] / df['close'].shift(26)) # 1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3adf62f",
   "metadata": {},
   "source": [
    "    Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8d329f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "lookback = 20  # 20 bars = 5 hours of 15min data\n",
    "\n",
    "df[\"volume_ma\"] = df[\"volume\"].rolling(window=lookback).mean()\n",
    "df[\"volume_ratio\"] = df[\"volume\"] / df[\"volume_ma\"]\n",
    "\n",
    "# Clean up\n",
    "df = df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac87fe2",
   "metadata": {},
   "source": [
    "    RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0a40544e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI Statistics:\n",
      "count    32008.000000\n",
      "mean        49.333163\n",
      "std         19.061507\n",
      "min          0.000000\n",
      "25%         35.177163\n",
      "50%         49.368087\n",
      "75%         63.157895\n",
      "max        100.000000\n",
      "Name: rsi_14, dtype: float64\n",
      "\n",
      "Overbought (>70): 4874 bars\n",
      "Oversold (<30): 5469 bars\n"
     ]
    }
   ],
   "source": [
    "# RSI calculation\n",
    "def calculate_rsi(data, period=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Add RSI with standard 14-period\n",
    "df[\"rsi_14\"] = calculate_rsi(df[\"close\"], period=14)\n",
    "\n",
    "# Clean up\n",
    "df = df.dropna(how=\"any\")\n",
    "\n",
    "# Check the feature\n",
    "print(\"RSI Statistics:\")\n",
    "print(df[\"rsi_14\"].describe())\n",
    "print(f\"\\nOverbought (>70): {(df['rsi_14'] > 70).sum()} bars\")\n",
    "print(f\"Oversold (<30): {(df['rsi_14'] < 30).sum()} bars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5488c",
   "metadata": {},
   "source": [
    "    MACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "51980bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACD Statistics:\n",
      "          macd_norm  macd_hist_norm\n",
      "count  32008.000000    32008.000000\n",
      "mean       0.000345        0.000005\n",
      "std        0.007443        0.002041\n",
      "min       -0.049496       -0.016980\n",
      "25%       -0.003475       -0.000954\n",
      "50%        0.000006       -0.000060\n",
      "75%        0.003886        0.000902\n",
      "max        0.045701        0.016795\n"
     ]
    }
   ],
   "source": [
    "# MACD calculation\n",
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    ema_fast = data.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = data.ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    macd_histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, macd_histogram\n",
    "\n",
    "df[\"macd\"], df[\"macd_signal\"], df[\"macd_hist\"] = calculate_macd(df[\"close\"])\n",
    "\n",
    "# Normalize MACD features by price (make them relative)\n",
    "df[\"macd_norm\"] = df[\"macd\"] / df[\"close\"]\n",
    "df[\"macd_hist_norm\"] = df[\"macd_hist\"] / df[\"close\"]\n",
    "\n",
    "# Clean up\n",
    "df = df.dropna(how=\"any\")\n",
    "\n",
    "print(\"MACD Statistics:\")\n",
    "print(df[[\"macd_norm\", \"macd_hist_norm\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05447ebd",
   "metadata": {},
   "source": [
    "    Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c621e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving averages (just for calculation, not direct features)\n",
    "df[\"5ma\"] = df[\"close\"].rolling(5).mean()\n",
    "df[\"10ma\"] = df[\"close\"].rolling(10).mean()\n",
    "df[\"50ma\"] = df[\"close\"].rolling(50).mean()\n",
    "df[\"200ma\"] = df[\"close\"].rolling(200).mean()  # Fixed: was 100\n",
    "\n",
    "# RELATIVE features (normalized by price)\n",
    "df[\"5_10_ma_diff_pct\"] = (df[\"5ma\"] - df[\"10ma\"]) / df[\"close\"]\n",
    "df[\"close_5ma_diff_pct\"] = (df[\"close\"] - df[\"5ma\"]) / df[\"close\"]\n",
    "df[\"close_10ma_diff_pct\"] = (df[\"close\"] - df[\"10ma\"]) / df[\"close\"]\n",
    "df[\"golden_cross_pct\"] = (df[\"50ma\"] - df[\"200ma\"]) / df[\"close\"]\n",
    "\n",
    "# Additional useful MA features\n",
    "df[\"close_50ma_diff_pct\"] = (df[\"close\"] - df[\"50ma\"]) / df[\"close\"]\n",
    "df[\"close_200ma_diff_pct\"] = (df[\"close\"] - df[\"200ma\"]) / df[\"close\"]\n",
    "\n",
    "# Slope/momentum of MAs (rate of change)\n",
    "df[\"5ma_roc\"] = df[\"5ma\"].pct_change(periods=5)\n",
    "df[\"50ma_roc\"] = df[\"50ma\"].pct_change(periods=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1c0c5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-Low range features\n",
    "df[\"hl_range\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"]  # Normalized\n",
    "df[\"hl_range_ma\"] = df[\"hl_range\"].rolling(20).mean()\n",
    "df[\"hl_range_ratio\"] = df[\"hl_range\"] / df[\"hl_range_ma\"]  # Current vs average\n",
    "\n",
    "# Close position within bar\n",
    "df[\"close_position\"] = (df[\"close\"] - df[\"low\"]) / (df[\"high\"] - df[\"low\"])  # 0 to 1\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0dc89c",
   "metadata": {},
   "source": [
    "    Candlestick/Microstructure Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "52299c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added microstructure features. New shape: (31661, 59)\n"
     ]
    }
   ],
   "source": [
    "# === CANDLESTICK BODY & WICKS ===\n",
    "df[\"body_size\"] = abs(df[\"close\"] - df[\"open\"]) / df[\"close\"]\n",
    "df[\"upper_wick\"] = (df[\"high\"] - df[[\"open\", \"close\"]].max(axis=1)) / df[\"close\"]\n",
    "df[\"lower_wick\"] = (df[[\"open\", \"close\"]].min(axis=1) - df[\"low\"]) / df[\"close\"]\n",
    "\n",
    "# Body direction\n",
    "df[\"body_direction\"] = np.sign(df[\"close\"] - df[\"open\"])\n",
    "\n",
    "# === OPEN-CLOSE RELATIONSHIPS ===\n",
    "df[\"open_close_diff\"] = (df[\"close\"] - df[\"open\"]) / df[\"close\"]\n",
    "df[\"high_close_diff\"] = (df[\"high\"] - df[\"close\"]) / df[\"close\"]\n",
    "df[\"low_close_diff\"] = (df[\"close\"] - df[\"low\"]) / df[\"close\"]\n",
    "\n",
    "# === VOLUME-WEIGHTED PRICE (VWAP) ===\n",
    "df[\"vwap_5\"] = (df[\"close\"] * df[\"volume\"]).rolling(5).sum() / df[\"volume\"].rolling(5).sum()\n",
    "df[\"vwap_10\"] = (df[\"close\"] * df[\"volume\"]).rolling(10).sum() / df[\"volume\"].rolling(10).sum()\n",
    "df[\"vwap_5_diff\"] = (df[\"close\"] - df[\"vwap_5\"]) / df[\"close\"]\n",
    "df[\"vwap_10_diff\"] = (df[\"close\"] - df[\"vwap_10\"]) / df[\"close\"]\n",
    "\n",
    "# === MOMENTUM OF MICROSTRUCTURE ===\n",
    "df[\"close_position_change\"] = df[\"close_position\"].diff()\n",
    "df[\"body_size_ratio\"] = df[\"body_size\"] / df[\"body_size\"].rolling(10).mean()\n",
    "\n",
    "# === BUYING/SELLING PRESSURE PROXIES ===\n",
    "# Approximation: If close near high = buying pressure\n",
    "df[\"buying_pressure\"] = (df[\"close\"] - df[\"low\"]) / (df[\"high\"] - df[\"low\"] + 1e-10)\n",
    "df[\"selling_pressure\"] = (df[\"high\"] - df[\"close\"]) / (df[\"high\"] - df[\"low\"] + 1e-10)\n",
    "\n",
    "# Clean up\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Added microstructure features. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac01008",
   "metadata": {},
   "source": [
    "#### Creating Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9baa5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"close_log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "df = df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7525a",
   "metadata": {},
   "source": [
    "    Shift to avoid lookahead bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3da4c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df[\"shifted_log_return\"] = df[\"close_log_return\"].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "51d97990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"close_log_return_lag1\"] = df[\"shifted_log_return\"].shift(1)\n",
    "df[\"close_log_return_lag2\"] = df[\"shifted_log_return\"].shift(2)\n",
    "df[\"close_log_return_lag3\"] = df[\"shifted_log_return\"].shift(3)\n",
    "df = df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0deda",
   "metadata": {},
   "source": [
    "#### Visualization discarded for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d4789163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from scipy import stats\n",
    "\n",
    "# # Assuming your data is loaded\n",
    "# # df has columns: ['open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "# # Calculate log returns (1-bar forward)\n",
    "# df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "# # Remove NaN\n",
    "# df = df.dropna()\n",
    "\n",
    "# # ============================================\n",
    "# # 1. BASIC STATISTICS\n",
    "# # ============================================\n",
    "# print(\"=\"*60)\n",
    "# print(\"LOG RETURNS DISTRIBUTION ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"Total samples: {len(df)}\")\n",
    "# print(f\"Mean: {df['log_return'].mean():.6f}\")\n",
    "# print(f\"Std Dev: {df['log_return'].std():.6f}\")\n",
    "# print(f\"Skewness: {df['log_return'].skew():.4f}\")\n",
    "# print(f\"Kurtosis: {df['log_return'].kurtosis():.4f}\")\n",
    "# print(f\"Min: {df['log_return'].min():.4f}\")\n",
    "# print(f\"Max: {df['log_return'].max():.4f}\")\n",
    "# print()\n",
    "\n",
    "# # ============================================\n",
    "# # 2. PERCENTILE ANALYSIS (CRITICAL)\n",
    "# # ============================================\n",
    "# percentiles = [1, 5, 10, 20, 25, 40, 50, 60, 75, 80, 90, 95, 99]\n",
    "# print(\"PERCENTILE BREAKDOWN:\")\n",
    "# print(\"-\" * 60)\n",
    "# for p in percentiles:\n",
    "#     val = np.percentile(df['log_return'], p)\n",
    "#     print(f\"{p:3d}th percentile: {val:8.4f} ({val*100:6.2f}%)\")\n",
    "# print()\n",
    "\n",
    "# # ============================================\n",
    "# # 3. PROPOSED CLASS BOUNDARIES\n",
    "# # ============================================\n",
    "# # Based on symmetric percentiles\n",
    "# p10 = np.percentile(df['log_return'], 10)  # Class 4: Big Down\n",
    "# p30 = np.percentile(df['log_return'], 30)  # Class 2: Small Down\n",
    "# p70 = np.percentile(df['log_return'], 70)  # Class 1: Small Up\n",
    "# p90 = np.percentile(df['log_return'], 90)  # Class 3: Big Up\n",
    "\n",
    "# print(\"PROPOSED CLASS BOUNDARIES (Percentile-based):\")\n",
    "# print(\"-\" * 60)\n",
    "# print(f\"Class 4 (Big Down):    return < {p10:.4f} ({p10*100:.2f}%)\")\n",
    "# print(f\"Class 2 (Small Down):  {p10:.4f} to {p30:.4f}\")\n",
    "# print(f\"Class 0 (Neutral):     {p30:.4f} to {p70:.4f}\")\n",
    "# print(f\"Class 1 (Small Up):    {p70:.4f} to {p90:.4f}\")\n",
    "# print(f\"Class 3 (Big Up):      return > {p90:.4f} ({p90*100:.2f}%)\")\n",
    "# print()\n",
    "\n",
    "# # Alternative: Fixed percentage boundaries\n",
    "# print(\"ALTERNATIVE: FIXED PERCENTAGE BOUNDARIES:\")\n",
    "# print(\"-\" * 60)\n",
    "# print(f\"Class 4 (Big Down):    return < -0.50%\")\n",
    "# print(f\"Class 2 (Small Down):  -0.50% to -0.15%\")\n",
    "# print(f\"Class 0 (Neutral):     -0.15% to +0.15%\")\n",
    "# print(f\"Class 1 (Small Up):    +0.15% to +0.50%\")\n",
    "# print(f\"Class 3 (Big Up):      return > +0.50%\")\n",
    "# print()\n",
    "\n",
    "# # Count samples in fixed boundaries\n",
    "# class_4_count = len(df[df['log_return'] < -0.005])\n",
    "# class_2_count = len(df[(df['log_return'] >= -0.005) & (df['log_return'] < -0.0015)])\n",
    "# class_0_count = len(df[(df['log_return'] >= -0.0015) & (df['log_return'] < 0.0015)])\n",
    "# class_1_count = len(df[(df['log_return'] >= 0.0015) & (df['log_return'] < 0.005)])\n",
    "# class_3_count = len(df[df['log_return'] >= 0.005])\n",
    "\n",
    "# print(\"Sample counts with fixed boundaries:\")\n",
    "# print(f\"Class 0 (Neutral): {class_0_count} ({class_0_count/len(df)*100:.1f}%)\")\n",
    "# print(f\"Class 1 (Small Up): {class_1_count} ({class_1_count/len(df)*100:.1f}%)\")\n",
    "# print(f\"Class 2 (Small Down): {class_2_count} ({class_2_count/len(df)*100:.1f}%)\")\n",
    "# print(f\"Class 3 (Big Up): {class_3_count} ({class_3_count/len(df)*100:.1f}%)\")\n",
    "# print(f\"Class 4 (Big Down): {class_4_count} ({class_4_count/len(df)*100:.1f}%)\")\n",
    "# print()\n",
    "\n",
    "# # ============================================\n",
    "# # 4. VISUALIZATION\n",
    "# # ============================================\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Histogram with KDE\n",
    "# axes[0, 0].hist(df['log_return'], bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "# axes[0, 0].axvline(df['log_return'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"log_return\"].mean():.4f}')\n",
    "# axes[0, 0].axvline(0, color='black', linestyle='-', alpha=0.3, label='Zero')\n",
    "# axes[0, 0].set_xlabel('Log Return')\n",
    "# axes[0, 0].set_ylabel('Frequency')\n",
    "# axes[0, 0].set_title('Distribution of Log Returns')\n",
    "# axes[0, 0].legend()\n",
    "# axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# # Q-Q Plot (Test for normality)\n",
    "# stats.probplot(df['log_return'], dist=\"norm\", plot=axes[0, 1])\n",
    "# axes[0, 1].set_title('Q-Q Plot (Normality Test)')\n",
    "# axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# # Boxplot\n",
    "# axes[1, 0].boxplot(df['log_return'], vert=True)\n",
    "# axes[1, 0].set_ylabel('Log Return')\n",
    "# axes[1, 0].set_title('Boxplot (Outlier Detection)')\n",
    "# axes[1, 0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "# axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# # Class boundaries visualization\n",
    "# axes[1, 1].hist(df['log_return'], bins=100, alpha=0.5, color='gray', edgecolor='black')\n",
    "# axes[1, 1].axvline(p10, color='red', linestyle='--', linewidth=2, label=f'10th: {p10:.4f}')\n",
    "# axes[1, 1].axvline(p30, color='orange', linestyle='--', linewidth=2, label=f'30th: {p30:.4f}')\n",
    "# axes[1, 1].axvline(p70, color='green', linestyle='--', linewidth=2, label=f'70th: {p70:.4f}')\n",
    "# axes[1, 1].axvline(p90, color='blue', linestyle='--', linewidth=2, label=f'90th: {p90:.4f}')\n",
    "# axes[1, 1].set_xlabel('Log Return')\n",
    "# axes[1, 1].set_ylabel('Frequency')\n",
    "# axes[1, 1].set_title('Proposed Class Boundaries (Percentile-based)')\n",
    "# axes[1, 1].legend()\n",
    "# axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('log_returns_distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # ============================================\n",
    "# # 5. NORMALITY TEST\n",
    "# # ============================================\n",
    "# # Jarque-Bera test\n",
    "# jb_stat, jb_pvalue = stats.jarque_bera(df['log_return'])\n",
    "# print(\"NORMALITY TESTS:\")\n",
    "# print(\"-\" * 60)\n",
    "# print(f\"Jarque-Bera statistic: {jb_stat:.2f}\")\n",
    "# print(f\"Jarque-Bera p-value: {jb_pvalue:.6f}\")\n",
    "# if jb_pvalue < 0.05:\n",
    "#     print(\"→ Returns are NOT normally distributed (reject null)\")\n",
    "# else:\n",
    "#     print(\"→ Returns appear normally distributed\")\n",
    "# print()\n",
    "\n",
    "# # ============================================\n",
    "# # 6. AUTOCORRELATION CHECK\n",
    "# # ============================================\n",
    "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "# plot_acf(df['log_return'].dropna(), lags=50, ax=axes[0])\n",
    "# axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "# plot_pacf(df['log_return'].dropna(), lags=50, ax=axes[1])\n",
    "# axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('autocorrelation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# print(\"ACF/PACF analysis saved. Check for:\")\n",
    "# print(\"- Significant lags → momentum/mean reversion patterns\")\n",
    "# print(\"- All lags near zero → random walk (no predictability)\")\n",
    "# print()\n",
    "\n",
    "# # ============================================\n",
    "# # 7. TIME-BASED ANALYSIS\n",
    "# # ============================================\n",
    "# # Assuming you have datetime index\n",
    "# if 'datetime' in df.columns or isinstance(df.index, pd.DatetimeIndex):\n",
    "#     if 'datetime' not in df.columns:\n",
    "#         df['datetime'] = df.index\n",
    "    \n",
    "#     df['hour'] = pd.to_datetime(df['datetime']).dt.hour\n",
    "#     df['year'] = pd.to_datetime(df['datetime']).dt.year\n",
    "    \n",
    "#     print(\"VOLATILITY BY HOUR (Intraday Pattern):\")\n",
    "#     print(\"-\" * 60)\n",
    "#     hourly_stats = df.groupby('hour')['log_return'].agg(['mean', 'std', 'count'])\n",
    "#     print(hourly_stats)\n",
    "#     print()\n",
    "    \n",
    "#     print(\"ANNUAL STATISTICS (Regime Changes):\")\n",
    "#     print(\"-\" * 60)\n",
    "#     yearly_stats = df.groupby('year')['log_return'].agg(['mean', 'std', 'count'])\n",
    "#     print(yearly_stats)\n",
    "#     print()\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"ANALYSIS COMPLETE\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b1518",
   "metadata": {},
   "source": [
    "#### Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea1971",
   "metadata": {},
   "source": [
    "    Creating Target and Deciding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15b8db",
   "metadata": {},
   "source": [
    "     Top 10 features by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "df9f6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\n",
    "   \n",
    "    \"hl_ratio\",              # 26.9% - #1\n",
    "    \"body_size\",             # 13.0% - #2\n",
    "    \"hour_cos\",              # 10.5% - #3\n",
    "    \"volume_ratio\",          # 9.4%  - #4\n",
    "    \"hour_sin\",              # 8.1%  - #5\n",
    "    \"high_close_diff\",       # 7.7%  - #6\n",
    "    \"close_5ma_diff_pct\",    # 5.7%  - #7\n",
    "    \"close_log_return_lag1\", # 4.1%  - #8\n",
    "    \"upper_shadow\",          # 3.2%  - #9\n",
    "    \"lower_shadow\"           # 3.0%  - #10\n",
    "]]\n",
    "# y = (df[\"shifted_log_return\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "93ac529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    12316\n",
      "1     6543\n",
      "2     6365\n",
      "3     3732\n",
      "4     2700\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate expanding percentiles on PAST data only\n",
    "df['p10'] = df['shifted_log_return'].expanding(min_periods=1000).quantile(0.10).shift(1)\n",
    "df['p30'] = df['shifted_log_return'].expanding(min_periods=1000).quantile(0.30).shift(1)\n",
    "df['p70'] = df['shifted_log_return'].expanding(min_periods=1000).quantile(0.70).shift(1)\n",
    "df['p90'] = df['shifted_log_return'].expanding(min_periods=1000).quantile(0.90).shift(1)\n",
    "\n",
    "def label_class_expanding(row):\n",
    "    ret = row['shifted_log_return']\n",
    "    p10, p30, p70, p90 = row['p10'], row['p30'], row['p70'], row['p90']\n",
    "    \n",
    "    if ret < p10: return 4      # Big Down\n",
    "    elif ret < p30: return 2    # Small Down\n",
    "    elif ret < p70: return 0    # Neutral\n",
    "    elif ret < p90: return 1    # Small Up\n",
    "    else: return 3              # Big Up\n",
    "\n",
    "df['target'] = df.apply(label_class_expanding, axis=1)\n",
    "\n",
    "# Drop the percentile columns (don't use as features)\n",
    "df = df.drop(columns=['p10', 'p30', 'p70', 'p90'])\n",
    "\n",
    "# Drop NaN\n",
    "df = df.dropna()\n",
    "\n",
    "print(df['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3dad6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de4da5",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b294bb5",
   "metadata": {},
   "source": [
    "#### Best model after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ec9cd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# # Step 1: 60% train, 40% remaining\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, shuffle=False)\n",
    "\n",
    "# # Step 2: Split 40% into 50/50 → each gets 20% of original\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# model = RandomForestClassifier(\n",
    "#     n_estimators=500,\n",
    "#     max_depth=3,\n",
    "#     min_samples_split=100,\n",
    "#     min_samples_leaf=50,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"Train score: {accuracy_score(y_train, model.predict(X_train)):.4f}\")\n",
    "# print(f\"Val score: {accuracy_score(y_val, model.predict(X_val)):.4f}\")\n",
    "# print(f\"Test score: {accuracy_score(y_test, model.predict(X_test)):.4f}\")\n",
    "# print(f\"\\nTest Confusion Matrix:\\n{confusion_matrix(y_test, model.predict(X_test))}\")\n",
    "# print(f\"\\nClassification Report:\\n{classification_report(y_test, model.predict(X_test))}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4942e5",
   "metadata": {},
   "source": [
    "#### Walk forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c27a912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Training Set: 2020-10-19 10:00:00+05:30 to 2022-12-30 15:30:00+05:30\n",
      "Training samples: 13456\n",
      "\n",
      "Walk-Forward Set: 2023-01-02 09:30:00+05:30 to 2025-12-12 15:15:00+05:30\n",
      "Walk-forward samples: 18200\n",
      "\n",
      "Training period: ~42.5% of data\n"
     ]
    }
   ],
   "source": [
    "# Define the initial training cutoff date\n",
    "train_end_date = '2022-12-31'\n",
    "\n",
    "# Split into initial training set and walk-forward test set\n",
    "X_train_initial = X[:train_end_date]\n",
    "y_train_initial = y[:train_end_date]\n",
    "\n",
    "X_walkforward = X[train_end_date:]\n",
    "y_walkforward = y[train_end_date:]\n",
    "\n",
    "print(f\"Initial Training Set: {X_train_initial.index[0]} to {X_train_initial.index[-1]}\")\n",
    "print(f\"Training samples: {len(X_train_initial)}\")\n",
    "print(f\"\\nWalk-Forward Set: {X_walkforward.index[0]} to {X_walkforward.index[-1]}\")\n",
    "print(f\"Walk-forward samples: {len(X_walkforward)}\")\n",
    "print(f\"\\nTraining period: ~{len(X_train_initial) / len(X) * 100:.1f}% of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2416424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest on initial training set...\n",
      "✓ Model training complete\n",
      "\n",
      "Training accuracy: 0.3396\n"
     ]
    }
   ],
   "source": [
    "# Train initial Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=3,\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=50,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest on initial training set...\")\n",
    "model.fit(X_train_initial, y_train_initial)\n",
    "print(\"✓ Model training complete\")\n",
    "\n",
    "# Quick sanity check\n",
    "train_score = model.score(X_train_initial, y_train_initial)\n",
    "print(f\"\\nTraining accuracy: {train_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ee3f0dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Period: 2023-01-02 09:30:00+05:30 to 2025-03-28 15:30:00+05:30\n",
      "Test samples: 13847\n",
      "\n",
      "Accuracy on first 3 months (Jan-Mar 2023): 0.3783\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.64      0.59      5756\n",
      "           1       0.26      0.19      0.22      2815\n",
      "           2       0.26      0.09      0.14      2786\n",
      "           3       0.15      0.13      0.14      1269\n",
      "           4       0.20      0.50      0.28      1221\n",
      "\n",
      "    accuracy                           0.38     13847\n",
      "   macro avg       0.28      0.31      0.27     13847\n",
      "weighted avg       0.37      0.38      0.36     13847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get first 3 months of walk-forward data for testing\n",
    "test_end_date = '2025-03-31'\n",
    "\n",
    "X_test_3m = X_walkforward[:test_end_date]\n",
    "y_test_3m = y_walkforward[:test_end_date]\n",
    "\n",
    "print(f\"Test Period: {X_test_3m.index[0]} to {X_test_3m.index[-1]}\")\n",
    "print(f\"Test samples: {len(X_test_3m)}\")\n",
    "\n",
    "# Predict on first 3 months\n",
    "y_pred_3m = model.predict(X_test_3m)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_3m = accuracy_score(y_test_3m, y_pred_3m)\n",
    "print(f\"\\nAccuracy on first 3 months (Jan-Mar 2023): {accuracy_3m:.4f}\")\n",
    "\n",
    "# Show classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_3m, y_pred_3m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c5f8e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strategy: Trade only Class 3 & 4 with >20% confidence ===\n",
      "\n",
      "Trades executed:\n",
      "  Long (Class 3): 1050\n",
      "  Short (Class 4): 3107\n",
      "  No position: 9690\n",
      "\n",
      "Gross Return: -0.1721 (-17.21%)\n",
      "Transaction Costs: 1.2052\n",
      "Net Return: -1.3773 (-137.73%)\n",
      "Sharpe Ratio (Annualized): -1.6740\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and probabilities for the 3-month test period\n",
    "y_pred_3m = model.predict(X_test_3m)\n",
    "y_proba_3m = model.predict_proba(X_test_3m)\n",
    "\n",
    "# Create results dataframe\n",
    "results_3m = pd.DataFrame({\n",
    "    'actual': y_test_3m.values,\n",
    "    'predicted': y_pred_3m\n",
    "}, index=y_test_3m.index)\n",
    "\n",
    "# Add probabilities for each class\n",
    "for i in range(5):\n",
    "    results_3m[f'prob_class_{i}'] = y_proba_3m[:, i]\n",
    "\n",
    "# Get actual returns\n",
    "results_3m['actual_return'] = df.loc[results_3m.index, 'shifted_log_return']\n",
    "\n",
    "# Trading logic: Only trade on class 3 (Big Up) and class 4 (Big Down) with >20% confidence\n",
    "results_3m['position'] = 0  # Default: no position\n",
    "\n",
    "# Long when predicted class 3 with >20% confidence\n",
    "long_signal = (results_3m['predicted'] == 3) & (results_3m['prob_class_3'] > 0.20)\n",
    "results_3m.loc[long_signal, 'position'] = 1\n",
    "\n",
    "# Short when predicted class 4 with >20% confidence\n",
    "short_signal = (results_3m['predicted'] == 4) & (results_3m['prob_class_4'] > 0.20)\n",
    "results_3m.loc[short_signal, 'position'] = -1\n",
    "\n",
    "# Strategy returns\n",
    "results_3m['strategy_return'] = results_3m['position'] * results_3m['actual_return']\n",
    "\n",
    "# Transaction costs: 0.04% per round trip\n",
    "transaction_cost = 0.0004\n",
    "results_3m['position_change'] = results_3m['position'].diff().fillna(results_3m['position'])\n",
    "results_3m['trade_occurred'] = (results_3m['position_change'] != 0).astype(int)\n",
    "results_3m['transaction_cost'] = results_3m['trade_occurred'] * transaction_cost\n",
    "results_3m['strategy_return_net'] = results_3m['strategy_return'] - results_3m['transaction_cost']\n",
    "\n",
    "# Performance metrics\n",
    "total_return_gross = results_3m['strategy_return'].sum()\n",
    "total_return_net = results_3m['strategy_return_net'].sum()\n",
    "sharpe_ratio_net = results_3m['strategy_return_net'].mean() / results_3m['strategy_return_net'].std() * np.sqrt(252 * 26)\n",
    "\n",
    "print(\"=== Strategy: Trade only Class 3 & 4 with >20% confidence ===\")\n",
    "print(f\"\\nTrades executed:\")\n",
    "print(f\"  Long (Class 3): {long_signal.sum()}\")\n",
    "print(f\"  Short (Class 4): {short_signal.sum()}\")\n",
    "print(f\"  No position: {(results_3m['position'] == 0).sum()}\")\n",
    "print(f\"\\nGross Return: {total_return_gross:.4f} ({total_return_gross*100:.2f}%)\")\n",
    "print(f\"Transaction Costs: {results_3m['transaction_cost'].sum():.4f}\")\n",
    "print(f\"Net Return: {total_return_net:.4f} ({total_return_net*100:.2f}%)\")\n",
    "print(f\"Sharpe Ratio (Annualized): {sharpe_ratio_net:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4f9dcc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confidence Threshold Analysis (21% - 40%) ===\n",
      "\n",
      " confidence  num_trades  num_longs  num_shorts  gross_return  transaction_costs  net_return  sharpe_ratio\n",
      "       0.21        4156       1050        3106     -0.167701             1.2060   -1.373701     -1.669622\n",
      "       0.22        4061       1027        3034     -0.161931             1.1948   -1.356731     -1.659974\n",
      "       0.23        3733        919        2814     -0.215646             1.1400   -1.355646     -1.691012\n",
      "       0.24        3387        814        2573     -0.420658             1.0684   -1.489058     -1.897045\n",
      "       0.25        3003        673        2330     -0.248975             1.0056   -1.254575     -1.648454\n",
      "       0.26        2521        442        2079     -0.194450             0.9428   -1.137250     -1.609825\n",
      "       0.27        1879        129        1750     -0.497937             0.8116   -1.309537     -2.089652\n",
      "       0.28        1501          1        1500     -0.663040             0.6652   -1.328240     -2.271845\n",
      "       0.29        1258          0        1258     -0.591278             0.5976   -1.188878     -2.180403\n",
      "       0.30        1043          0        1043     -0.583823             0.5432   -1.127023     -2.314042\n",
      "       0.31         890          0         890     -0.344290             0.4960   -0.840290     -1.847025\n",
      "       0.32         748          0         748     -0.199737             0.4344   -0.634137     -1.485147\n",
      "       0.33         650          0         650     -0.332753             0.3912   -0.723953     -1.853389\n",
      "       0.34         541          0         541     -0.315481             0.3400   -0.655481     -1.913784\n",
      "       0.35         412          0         412     -0.280412             0.2736   -0.554012     -1.864086\n",
      "       0.36         314          0         314     -0.278839             0.2128   -0.491639     -1.852870\n",
      "       0.37         268          0         268     -0.210787             0.1856   -0.396387     -1.621335\n",
      "       0.38         245          0         245     -0.135791             0.1704   -0.306191     -1.322013\n",
      "       0.39         222          0         222     -0.137983             0.1552   -0.293183     -1.312114\n",
      "       0.40         194          0         194     -0.070083             0.1376   -0.207683     -1.026226\n",
      "\n",
      "=== Best Configurations ===\n",
      "\n",
      "Best Sharpe Ratio: 0.40 (Sharpe: -1.0262, Return: -20.77%)\n",
      "\n",
      "Best Net Return: 0.40 (Return: -20.77%, Sharpe: -1.0262)\n"
     ]
    }
   ],
   "source": [
    "# Test different confidence thresholds\n",
    "confidence_thresholds = np.arange(0.21, 0.41, 0.01)  # 21% to 40%\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for conf_threshold in confidence_thresholds:\n",
    "    # Create temporary results\n",
    "    temp_results = results_3m.copy()\n",
    "    \n",
    "    # Reset positions\n",
    "    temp_results['position'] = 0\n",
    "    \n",
    "    # Long when predicted class 3 with >= threshold confidence\n",
    "    long_signal = (temp_results['predicted'] == 3) & (temp_results['prob_class_3'] >= conf_threshold)\n",
    "    temp_results.loc[long_signal, 'position'] = 1\n",
    "    \n",
    "    # Short when predicted class 4 with >= threshold confidence\n",
    "    short_signal = (temp_results['predicted'] == 4) & (temp_results['prob_class_4'] >= conf_threshold)\n",
    "    temp_results.loc[short_signal, 'position'] = -1\n",
    "    \n",
    "    # Calculate returns\n",
    "    temp_results['strategy_return'] = temp_results['position'] * temp_results['actual_return']\n",
    "    \n",
    "    # Transaction costs\n",
    "    temp_results['position_change'] = temp_results['position'].diff().fillna(temp_results['position'])\n",
    "    temp_results['trade_occurred'] = (temp_results['position_change'] != 0).astype(int)\n",
    "    temp_results['transaction_cost'] = temp_results['trade_occurred'] * transaction_cost\n",
    "    temp_results['strategy_return_net'] = temp_results['strategy_return'] - temp_results['transaction_cost']\n",
    "    \n",
    "    # Metrics\n",
    "    num_trades = (temp_results['position'] != 0).sum()\n",
    "    num_longs = (temp_results['position'] == 1).sum()\n",
    "    num_shorts = (temp_results['position'] == -1).sum()\n",
    "    gross_return = temp_results['strategy_return'].sum()\n",
    "    net_return = temp_results['strategy_return_net'].sum()\n",
    "    total_costs = temp_results['transaction_cost'].sum()\n",
    "    \n",
    "    if temp_results['strategy_return_net'].std() > 0:\n",
    "        sharpe = temp_results['strategy_return_net'].mean() / temp_results['strategy_return_net'].std() * np.sqrt(252 * 26)\n",
    "    else:\n",
    "        sharpe = 0\n",
    "    \n",
    "    results_summary.append({\n",
    "        'confidence': conf_threshold,\n",
    "        'num_trades': num_trades,\n",
    "        'num_longs': num_longs,\n",
    "        'num_shorts': num_shorts,\n",
    "        'gross_return': gross_return,\n",
    "        'transaction_costs': total_costs,\n",
    "        'net_return': net_return,\n",
    "        'sharpe_ratio': sharpe\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "print(\"=== Confidence Threshold Analysis (21% - 40%) ===\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Find best performing threshold\n",
    "best_sharpe_idx = summary_df['sharpe_ratio'].idxmax()\n",
    "best_return_idx = summary_df['net_return'].idxmax()\n",
    "\n",
    "print(f\"\\n=== Best Configurations ===\")\n",
    "print(f\"\\nBest Sharpe Ratio: {summary_df.loc[best_sharpe_idx, 'confidence']:.2f} \"\n",
    "      f\"(Sharpe: {summary_df.loc[best_sharpe_idx, 'sharpe_ratio']:.4f}, \"\n",
    "      f\"Return: {summary_df.loc[best_sharpe_idx, 'net_return']*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nBest Net Return: {summary_df.loc[best_return_idx, 'confidence']:.2f} \"\n",
    "      f\"(Return: {summary_df.loc[best_return_idx, 'net_return']*100:.2f}%, \"\n",
    "      f\"Sharpe: {summary_df.loc[best_return_idx, 'sharpe_ratio']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c8984",
   "metadata": {},
   "source": [
    "### Full Walk forward Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0a585930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward validation setup\n",
    "confidence_threshold = 0.26\n",
    "transaction_cost = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5718ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage for results\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "all_probabilities = []  # Store full probability arrays\n",
    "all_returns = []\n",
    "window_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f2ec6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string index to datetime BEFORE the loop\n",
    "X_walkforward.index = pd.to_datetime(X_walkforward.index)\n",
    "y_walkforward.index = pd.to_datetime(y_walkforward.index)\n",
    "\n",
    "# Get walk-forward period boundaries\n",
    "walkforward_start = X_walkforward.index[0]\n",
    "walkforward_end = X_walkforward.index[-1]\n",
    "current_date = walkforward_start\n",
    "window_num = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b035d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 09:30:00+05:30\n",
      "2025-12-12 15:15:00+05:30\n",
      "2023-01-02 09:30:00+05:30\n"
     ]
    }
   ],
   "source": [
    "print(walkforward_start)\n",
    "print(walkforward_end)\n",
    "print(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "30166c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Walk-Forward Validation (3-Month Windows, Retrain After Each) ===\n",
      "\n",
      "Window 1: 2023-01-02 to 2023-03-31 (1550 bars)\n",
      "  Retraining model with 15006 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 2: 2023-04-03 to 2023-06-30 (1500 bars)\n",
      "  Retraining model with 16506 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 3: 2023-07-03 to 2023-09-29 (1575 bars)\n",
      "  Retraining model with 18081 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 4: 2023-10-03 to 2024-01-02 (1530 bars)\n",
      "  Retraining model with 19611 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 5: 2024-01-02 to 2024-04-02 (1533 bars)\n",
      "  Retraining model with 21143 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 6: 2024-04-02 to 2024-07-02 (1508 bars)\n",
      "  Retraining model with 22650 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 7: 2024-07-02 to 2024-10-01 (1600 bars)\n",
      "  Retraining model with 24249 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 8: 2024-10-03 to 2025-01-02 (1530 bars)\n",
      "  Retraining model with 25779 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 9: 2025-01-02 to 2025-04-02 (1551 bars)\n",
      "  Retraining model with 27329 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 10: 2025-04-02 to 2025-07-02 (1526 bars)\n",
      "  Retraining model with 28854 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 11: 2025-07-02 to 2025-10-01 (1600 bars)\n",
      "  Retraining model with 30453 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "Window 12: 2025-10-03 to 2025-12-12 (1203 bars)\n",
      "  Retraining model with 31656 samples...\n",
      "  ✓ Model retrained\n",
      "\n",
      "=== Walk-Forward Loop Complete ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize storage for predictions only\n",
    "window_predictions_list = []\n",
    "\n",
    "print(\"=== Walk-Forward Validation (3-Month Windows, Retrain After Each) ===\\n\")\n",
    "\n",
    "while current_date < walkforward_end:\n",
    "    window_num += 1\n",
    "    \n",
    "    # Define 3-month test window\n",
    "    test_start = current_date\n",
    "    test_end = test_start + pd.DateOffset(months=3)\n",
    "    \n",
    "    # Get test data for this window\n",
    "    X_test_window = X_walkforward.loc[test_start:test_end]\n",
    "    y_test_window = y_walkforward.loc[test_start:test_end]\n",
    "    \n",
    "    if len(X_test_window) == 0:\n",
    "        break\n",
    "    \n",
    "    print(f\"Window {window_num}: {X_test_window.index[0].date()} to {X_test_window.index[-1].date()} ({len(X_test_window)} bars)\")\n",
    "    \n",
    "    # Predict on this window using current model\n",
    "    y_pred = model.predict(X_test_window)\n",
    "    y_proba = model.predict_proba(X_test_window)\n",
    "    \n",
    "    # Create predictions dataframe for this window\n",
    "    window_predictions = pd.DataFrame(index=X_test_window.index)\n",
    "    window_predictions['actual'] = y_test_window.values\n",
    "    window_predictions['predicted'] = y_pred\n",
    "    \n",
    "    # Add probability scores for all classes\n",
    "    for i in range(5):\n",
    "        window_predictions[f'prob_class_{i}'] = y_proba[:, i]\n",
    "    \n",
    "    # Save this window's predictions\n",
    "    window_predictions_list.append(window_predictions)\n",
    "    \n",
    "    # Retrain model: Add this window to training data\n",
    "    X_train_expanded = pd.concat([X_train_initial, X_walkforward.loc[walkforward_start:test_end]])\n",
    "    y_train_expanded = pd.concat([y_train_initial, y_walkforward.loc[walkforward_start:test_end]])\n",
    "    \n",
    "    print(f\"  Retraining model with {len(X_train_expanded)} samples...\")\n",
    "    model.fit(X_train_expanded, y_train_expanded)\n",
    "    print(f\"  ✓ Model retrained\\n\")\n",
    "    \n",
    "    # Move to next window\n",
    "    current_date = test_end\n",
    "\n",
    "print(\"=== Walk-Forward Loop Complete ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b0ebd7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions collected: 18206\n",
      "Date range: 2023-01-02 to 2025-12-12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine all window predictions into one dataframe\n",
    "all_predictions_df = pd.concat(window_predictions_list)\n",
    "\n",
    "print(f\"Total predictions collected: {len(all_predictions_df)}\")\n",
    "print(f\"Date range: {all_predictions_df.index[0].date()} to {all_predictions_df.index[-1].date()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fbe2ca48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>prob_class_0</th>\n",
       "      <th>prob_class_1</th>\n",
       "      <th>prob_class_2</th>\n",
       "      <th>prob_class_3</th>\n",
       "      <th>prob_class_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-02 09:30:00+05:30</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.084137</td>\n",
       "      <td>0.152246</td>\n",
       "      <td>0.147651</td>\n",
       "      <td>0.267892</td>\n",
       "      <td>0.348074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 09:45:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.105732</td>\n",
       "      <td>0.170619</td>\n",
       "      <td>0.181933</td>\n",
       "      <td>0.243290</td>\n",
       "      <td>0.298426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 10:00:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297611</td>\n",
       "      <td>0.216539</td>\n",
       "      <td>0.213443</td>\n",
       "      <td>0.164041</td>\n",
       "      <td>0.108365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 10:15:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318316</td>\n",
       "      <td>0.201793</td>\n",
       "      <td>0.216244</td>\n",
       "      <td>0.164189</td>\n",
       "      <td>0.099458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 10:30:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308753</td>\n",
       "      <td>0.206530</td>\n",
       "      <td>0.216059</td>\n",
       "      <td>0.164850</td>\n",
       "      <td>0.103808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 14:15:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.183210</td>\n",
       "      <td>0.205668</td>\n",
       "      <td>0.214045</td>\n",
       "      <td>0.196510</td>\n",
       "      <td>0.200568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 14:30:00+05:30</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.231879</td>\n",
       "      <td>0.220961</td>\n",
       "      <td>0.159948</td>\n",
       "      <td>0.155551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 14:45:00+05:30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.210920</td>\n",
       "      <td>0.230872</td>\n",
       "      <td>0.216655</td>\n",
       "      <td>0.167946</td>\n",
       "      <td>0.173606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 15:00:00+05:30</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.209406</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.197164</td>\n",
       "      <td>0.215359</td>\n",
       "      <td>0.178548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 15:15:00+05:30</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.179414</td>\n",
       "      <td>0.166206</td>\n",
       "      <td>0.279227</td>\n",
       "      <td>0.241027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18206 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           actual  predicted  prob_class_0  prob_class_1  \\\n",
       "datetime                                                                   \n",
       "2023-01-02 09:30:00+05:30       3          4      0.084137      0.152246   \n",
       "2023-01-02 09:45:00+05:30       0          4      0.105732      0.170619   \n",
       "2023-01-02 10:00:00+05:30       0          0      0.297611      0.216539   \n",
       "2023-01-02 10:15:00+05:30       0          0      0.318316      0.201793   \n",
       "2023-01-02 10:30:00+05:30       0          0      0.308753      0.206530   \n",
       "...                           ...        ...           ...           ...   \n",
       "2025-12-12 14:15:00+05:30       0          2      0.183210      0.205668   \n",
       "2025-12-12 14:30:00+05:30       2          1      0.231661      0.231879   \n",
       "2025-12-12 14:45:00+05:30       1          1      0.210920      0.230872   \n",
       "2025-12-12 15:00:00+05:30       1          3      0.209406      0.199524   \n",
       "2025-12-12 15:15:00+05:30       2          3      0.134125      0.179414   \n",
       "\n",
       "                           prob_class_2  prob_class_3  prob_class_4  \n",
       "datetime                                                             \n",
       "2023-01-02 09:30:00+05:30      0.147651      0.267892      0.348074  \n",
       "2023-01-02 09:45:00+05:30      0.181933      0.243290      0.298426  \n",
       "2023-01-02 10:00:00+05:30      0.213443      0.164041      0.108365  \n",
       "2023-01-02 10:15:00+05:30      0.216244      0.164189      0.099458  \n",
       "2023-01-02 10:30:00+05:30      0.216059      0.164850      0.103808  \n",
       "...                                 ...           ...           ...  \n",
       "2025-12-12 14:15:00+05:30      0.214045      0.196510      0.200568  \n",
       "2025-12-12 14:30:00+05:30      0.220961      0.159948      0.155551  \n",
       "2025-12-12 14:45:00+05:30      0.216655      0.167946      0.173606  \n",
       "2025-12-12 15:00:00+05:30      0.197164      0.215359      0.178548  \n",
       "2025-12-12 15:15:00+05:30      0.166206      0.279227      0.241027  \n",
       "\n",
       "[18206 rows x 7 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f238da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add actual returns to the predictions dataframe\n",
    "all_predictions_df['actual_return'] = df.loc[all_predictions_df.index, 'shifted_log_return'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "da3a9971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>prob_class_0</th>\n",
       "      <th>prob_class_1</th>\n",
       "      <th>prob_class_2</th>\n",
       "      <th>prob_class_3</th>\n",
       "      <th>prob_class_4</th>\n",
       "      <th>actual_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-02 09:30:00+05:30</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.084137</td>\n",
       "      <td>0.152246</td>\n",
       "      <td>0.147651</td>\n",
       "      <td>0.267892</td>\n",
       "      <td>0.348074</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 09:45:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.105732</td>\n",
       "      <td>0.170619</td>\n",
       "      <td>0.181933</td>\n",
       "      <td>0.243290</td>\n",
       "      <td>0.298426</td>\n",
       "      <td>-0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 10:00:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297611</td>\n",
       "      <td>0.216539</td>\n",
       "      <td>0.213443</td>\n",
       "      <td>0.164041</td>\n",
       "      <td>0.108365</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 10:15:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318316</td>\n",
       "      <td>0.201793</td>\n",
       "      <td>0.216244</td>\n",
       "      <td>0.164189</td>\n",
       "      <td>0.099458</td>\n",
       "      <td>0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02 10:30:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308753</td>\n",
       "      <td>0.206530</td>\n",
       "      <td>0.216059</td>\n",
       "      <td>0.164850</td>\n",
       "      <td>0.103808</td>\n",
       "      <td>-0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 14:15:00+05:30</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.183210</td>\n",
       "      <td>0.205668</td>\n",
       "      <td>0.214045</td>\n",
       "      <td>0.196510</td>\n",
       "      <td>0.200568</td>\n",
       "      <td>-0.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 14:30:00+05:30</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.231879</td>\n",
       "      <td>0.220961</td>\n",
       "      <td>0.159948</td>\n",
       "      <td>0.155551</td>\n",
       "      <td>-0.004480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 14:45:00+05:30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.210920</td>\n",
       "      <td>0.230872</td>\n",
       "      <td>0.216655</td>\n",
       "      <td>0.167946</td>\n",
       "      <td>0.173606</td>\n",
       "      <td>0.002127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 15:00:00+05:30</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.209406</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.197164</td>\n",
       "      <td>0.215359</td>\n",
       "      <td>0.178548</td>\n",
       "      <td>0.004278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12 15:15:00+05:30</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.179414</td>\n",
       "      <td>0.166206</td>\n",
       "      <td>0.279227</td>\n",
       "      <td>0.241027</td>\n",
       "      <td>-0.003429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18206 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           actual  predicted  prob_class_0  prob_class_1  \\\n",
       "datetime                                                                   \n",
       "2023-01-02 09:30:00+05:30       3          4      0.084137      0.152246   \n",
       "2023-01-02 09:45:00+05:30       0          4      0.105732      0.170619   \n",
       "2023-01-02 10:00:00+05:30       0          0      0.297611      0.216539   \n",
       "2023-01-02 10:15:00+05:30       0          0      0.318316      0.201793   \n",
       "2023-01-02 10:30:00+05:30       0          0      0.308753      0.206530   \n",
       "...                           ...        ...           ...           ...   \n",
       "2025-12-12 14:15:00+05:30       0          2      0.183210      0.205668   \n",
       "2025-12-12 14:30:00+05:30       2          1      0.231661      0.231879   \n",
       "2025-12-12 14:45:00+05:30       1          1      0.210920      0.230872   \n",
       "2025-12-12 15:00:00+05:30       1          3      0.209406      0.199524   \n",
       "2025-12-12 15:15:00+05:30       2          3      0.134125      0.179414   \n",
       "\n",
       "                           prob_class_2  prob_class_3  prob_class_4  \\\n",
       "datetime                                                              \n",
       "2023-01-02 09:30:00+05:30      0.147651      0.267892      0.348074   \n",
       "2023-01-02 09:45:00+05:30      0.181933      0.243290      0.298426   \n",
       "2023-01-02 10:00:00+05:30      0.213443      0.164041      0.108365   \n",
       "2023-01-02 10:15:00+05:30      0.216244      0.164189      0.099458   \n",
       "2023-01-02 10:30:00+05:30      0.216059      0.164850      0.103808   \n",
       "...                                 ...           ...           ...   \n",
       "2025-12-12 14:15:00+05:30      0.214045      0.196510      0.200568   \n",
       "2025-12-12 14:30:00+05:30      0.220961      0.159948      0.155551   \n",
       "2025-12-12 14:45:00+05:30      0.216655      0.167946      0.173606   \n",
       "2025-12-12 15:00:00+05:30      0.197164      0.215359      0.178548   \n",
       "2025-12-12 15:15:00+05:30      0.166206      0.279227      0.241027   \n",
       "\n",
       "                           actual_return  \n",
       "datetime                                  \n",
       "2023-01-02 09:30:00+05:30       0.006883  \n",
       "2023-01-02 09:45:00+05:30      -0.000762  \n",
       "2023-01-02 10:00:00+05:30       0.000038  \n",
       "2023-01-02 10:15:00+05:30       0.000191  \n",
       "2023-01-02 10:30:00+05:30      -0.000229  \n",
       "...                                  ...  \n",
       "2025-12-12 14:15:00+05:30      -0.000578  \n",
       "2025-12-12 14:30:00+05:30      -0.004480  \n",
       "2025-12-12 14:45:00+05:30       0.002127  \n",
       "2025-12-12 15:00:00+05:30       0.004278  \n",
       "2025-12-12 15:15:00+05:30      -0.003429  \n",
       "\n",
       "[18206 rows x 8 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f88976dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_df[\"acc\"] = (all_predictions_df[\"actual\"] == all_predictions_df[\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "db22ff8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc\n",
       "False    11419\n",
       "True      6787\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions_df[\"acc\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
